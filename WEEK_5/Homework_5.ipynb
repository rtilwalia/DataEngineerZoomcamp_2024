{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af705c4-16aa-41c5-a347-2bf68db4a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0adccc-8e98-4099-8eb8-5142ffa229e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 17:33:33 WARN Utils: Your hostname, Ritikas-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.100 instead (on interface en0)\n",
      "24/03/06 17:33:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/06 17:33:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/06 17:33:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d7d188-5ea0-475d-83f4-295592992926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc5e92a0-86be-4434-b602-ac9194376d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-06 17:15:41--  https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-10.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 2600:9000:26c0:dc00:b:20a5:b140:21, 2600:9000:26c0:8000:b:20a5:b140:21, 2600:9000:26c0:7e00:b:20a5:b140:21, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|2600:9000:26c0:dc00:b:20a5:b140:21|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18344035 (17M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘fhv_tripdata_2019-10.parquet’\n",
      "\n",
      "fhv_tripdata_2019-1 100%[===================>]  17.49M  43.7MB/s    in 0.4s    \n",
      "\n",
      "2024-03-06 17:15:41 (43.7 MB/s) - ‘fhv_tripdata_2019-10.parquet’ saved [18344035/18344035]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-10.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f44c8d7-a4e9-4ea1-96b2-d861250ae4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Parquet file\n",
    "parquet_file_path = \"fhv_tripdata_2019-10.parquet\"\n",
    "\n",
    "# Specify the path where you want to save the CSV file\n",
    "csv_file_path = \"fhv_tripdata_2019-10.csv\"\n",
    "\n",
    "# Read Parquet file into a Pandas DataFrame\n",
    "df = pd.read_parquet(parquet_file_path, engine=\"pyarrow\")  # Use \"fastparquet\" if you prefer\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(\"Conversion completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a5a0a5-be3f-4c59-a94f-2e0d88239ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1897857 fhv_tripdata_2019-10.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhv_tripdata_2019-10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f0966b-f664-4ad0-b7cb-5c4f9e818d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec7568b-63bb-412a-b028-aeef2311a1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dispatching_base_num', 'string'),\n",
       " ('pickup_datetime', 'string'),\n",
       " ('dropOff_datetime', 'string'),\n",
       " ('PUlocationID', 'string'),\n",
       " ('DOlocationID', 'string'),\n",
       " ('SR_Flag', 'string'),\n",
       " ('Affiliated_base_number', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b6537a6-8b58-421a-a146-27e0f83990b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B00009', pickup_datetime='2019-10-01 00:23:00', dropOff_datetime='2019-10-01 00:35:00', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number='B00009'),\n",
       " Row(dispatching_base_num='B00013', pickup_datetime='2019-10-01 00:11:29', dropOff_datetime='2019-10-01 00:13:22', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number='B00013'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:11:43', dropOff_datetime='2019-10-01 00:37:20', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:56:29', dropOff_datetime='2019-10-01 00:57:47', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number='B00014'),\n",
       " Row(dispatching_base_num='B00014', pickup_datetime='2019-10-01 00:23:09', dropOff_datetime='2019-10-01 00:28:27', PUlocationID='264.0', DOlocationID='264.0', SR_Flag=None, Affiliated_base_number='B00014')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8dfd618-df09-4528-9fae-47de21ccde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3cbfedc-1852-44e8-babf-5d903daa78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a524f9c9-1876-4f88-a6ba-bc705cc023f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab0209f-7362-48f3-8815-c8ec25a34eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dispatching_base_num', 'string'),\n",
       " ('pickup_datetime', 'timestamp'),\n",
       " ('dropoff_datetime', 'timestamp'),\n",
       " ('PULocationID', 'int'),\n",
       " ('DOLocationID', 'int'),\n",
       " ('SR_Flag', 'string'),\n",
       " ('Affiliated_base_number', 'string')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61519de9-ebc9-48c6-ab3e-c0109459445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24c16ad-db31-414c-b87b-6930ec544f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int, SR_Flag: string, Affiliated_base_number: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a3528d-77c3-43bf-b5a9-1789e2b5c7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8407de4-6e90-496e-bb74-48bc0af4ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "487385d8-f05d-4253-84c6-4bc722331333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"fhv_tripdata_2019-10.parquet\"\n",
    "df_main = spark.read.parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3904aac-1c70-4461-8504-2d3e5d98516e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffd3d1fc-7475-40c2-8c46-2c1c96281620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea6c3c2-972b-4afa-acaa-a624fc062616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62629"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many taxi trips were there on the 15th of October?\n",
    "\n",
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2019-10-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a9a702b-bdc2-44e2-b3ff-88cc7830451b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('fhv_2019_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a730c7d1-3fad-4150-adf0-03c6e782e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62629|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhv_2019_10\n",
    "WHERE \n",
    "    to_date(pickup_datetime) = '2019-10-15';\n",
    ";\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a1a525d-2147-442d-ab66-24b35e198e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b714afc-feff-4e6d-a08d-524a28a2bc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|     B03188         |2019-10-01 03:12:58|2019-10-01 04:24:16|        NULL|        NULL|   NULL|       B03188         |\n",
      "|              B00887|2019-10-03 08:39:04|2019-10-03 09:22:27|        NULL|        NULL|   NULL|                B00887|\n",
      "|              B00256|2019-10-02 12:23:48|2019-10-02 13:05:07|        NULL|        NULL|   NULL|                B00256|\n",
      "|              B02834|2019-10-01 18:20:47|2019-10-01 18:58:47|        NULL|        NULL|   NULL|                B02834|\n",
      "|     B01995         |2019-10-03 00:29:50|2019-10-03 00:35:53|        NULL|        NULL|   NULL|       B01995         |\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Longest trip for each day\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "    FROM \n",
    "    fhv_2019_10\n",
    "limit 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acfb2720-7283-4fc4-8427-80ed5a8f3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|pickup_date|         duration|\n",
      "+-----------+-----------------+\n",
      "| 2019-10-28|       3.786915E7|\n",
      "| 2019-10-11|       3.786915E7|\n",
      "| 2019-10-31|       5260346.45|\n",
      "| 2019-10-01|4207681.683333334|\n",
      "| 2019-10-17|         527640.0|\n",
      "| 2019-10-26|         527050.0|\n",
      "| 2019-10-30|87932.06666666667|\n",
      "| 2019-10-25|          63469.6|\n",
      "| 2019-10-02|46213.88333333333|\n",
      "| 2019-10-23|          44797.0|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 60) AS duration\n",
    "FROM \n",
    "    fhv_2019_10\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63499e34-afef-4b60-9fff-4d6677e380ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|pickup_date|max(duration)|\n",
      "+-----------+-------------+\n",
      "| 2019-10-28|   2272149000|\n",
      "| 2019-10-11|   2272149000|\n",
      "| 2019-10-31|    315620787|\n",
      "| 2019-10-01|    252460901|\n",
      "| 2019-10-17|     31658400|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('duration', df.dropoff_datetime.cast('long') - df.pickup_datetime.cast('long')) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "        .max('duration') \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6a6a3ef-90b1-4b62-b262-25e16c26f098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_main.registerTempTable('fhv_2019_10_main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a187343-5de3-431c-a509-265331a0d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|       264.0|       264.0|   NULL|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|       264.0|       264.0|   NULL|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|       264.0|       264.0|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|       264.0|       264.0|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|       264.0|       264.0|   NULL|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|       129.0|       129.0|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|        57.0|        57.0|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|       173.0|       173.0|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|       226.0|       226.0|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|        56.0|        56.0|   NULL|       B00021         |\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    * from fhv_2019_10_main\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5228482d-01cd-4fb3-864c-ae3847c878f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a01a0182-9d63-4d23-9aee-78aeef86a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/06 18:10:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/03/06 18:10:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/03/06 18:10:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/03/06 18:10:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/03/06 18:10:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "24/03/06 18:10:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "24/03/06 18:10:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "24/03/06 18:10:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "24/03/06 18:10:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_2.write.parquet('fhv_2019_10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c17b6a5-9d5d-4e5d-a690-82a17c0edbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.registerTempTable('fhv_2019_10_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b6067f6-1aee-44bd-8bad-1f656e3222ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|        NULL|        NULL|   NULL|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|        NULL|        NULL|   NULL|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|        NULL|        NULL|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|        NULL|        NULL|   NULL|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|        NULL|        NULL|   NULL|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|        NULL|        NULL|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|        NULL|        NULL|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|        NULL|        NULL|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|        NULL|        NULL|   NULL|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|        NULL|        NULL|   NULL|       B00021         |\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    * from fhv_2019_10_2\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
